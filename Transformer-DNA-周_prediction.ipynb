{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28d559c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, KBinsDiscretizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import datetime\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9171863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 29\n",
    "num_layers = 2\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2401eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutWord(x, window=1, length = seq_length):\n",
    "    seq2word = []\n",
    "    for i in range(length):\n",
    "        seq2word.append(x[i*window:i*window+window])\n",
    "    return \" \".join(seq2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36a80417",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df = pd.read_csv('predicted_data/1280.txt', header=None, names=['seq'])\n",
    "\n",
    "# 预处理数据\n",
    "x_test_spaced = x_test_df.applymap(cutWord)\n",
    "\n",
    "# 创建并适应vectorize_layer（假设你之前已经定义了d_model和vectorize_layer）\n",
    "# d_model = 10000  # 例如，假设d_model是10000\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=d_model, standardize='lower_and_strip_punctuation', split='whitespace', ngrams=None, output_mode='int')\n",
    "vectorize_layer.adapt(x_test_spaced)\n",
    "\n",
    "# 向量化数据\n",
    "x_test_vectorized = vectorize_layer(x_test_spaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecfcc2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RKA_bin(x):\n",
    "    if x >= 0.8:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "def MFE_bin(x):\n",
    "    if x >= -14.:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f795966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) # Shape = (..., seq_len_q, seq_len_k)\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # calculate matmul_qk_v\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights, matmul_qk\n",
    "\n",
    "#create multi-head attention layer\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        #Because for multi-head, head number * depth = multi-head\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #Set layers for q, k, v\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) # shape = (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights, matmul_qk= scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights, matmul_qk\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        ])\n",
    "        \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        #define layers\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "\n",
    "        attn_output, attn_weight, matmul_qk = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2, attn_weight, matmul_qk\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rate = 1/np.power(10000, (2*(i/2))/np.float32(d_model))\n",
    "    return pos*angle_rate\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :]\n",
    "                            ,d_model)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #how many encoder layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.attention_weights = {}\n",
    "        self.matmul_qks = {}\n",
    "        \n",
    "    def call(self, x, training, mask=None):\n",
    "        #attention_weights = {}\n",
    "        #encoding and position encoding\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block, matmul_qk = self.enc_layers[i](x, training, mask)\n",
    "            self.attention_weights[f'attentions_{i+1}'] = block\n",
    "            self.matmul_qks[f'matmul_qk_{i+1}'] = matmul_qk\n",
    "            \n",
    "        #attention_weights[f'decoder_layer{i+1}_block'] = block\n",
    "        return x, block # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    def get_attention(self):\n",
    "        return self.attention_weights\n",
    "    def get_matmul_qks(self):\n",
    "        return self.matmul_qks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "433af158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7, 29, 5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=5, num_heads=1,\n",
    "                         dff=dff, input_vocab_size=10,\n",
    "                         maximum_position_encoding=30)\n",
    "temp_input = vectorize_layer(x_test_spaced)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input[22:29], training=False, mask=None)\n",
    "sample_encoder_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ddf7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_length, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.3):\n",
    "    input = tf.keras.Input(shape=(seq_length,))\n",
    "    x , aws= Encoder(num_layers, d_model, num_heads, dff,\n",
    "                input_vocab_size, maximum_position_encoding, rate=rate)(input)\n",
    "    x = tf.keras.layers.Reshape((seq_length*d_model,))(x)\n",
    "    x = tf.keras.layers.Dense(seq_length, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    output = tf.squeeze(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8031e1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-1.8591526>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(seq_length=seq_length, num_layers= num_layers, d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, input_vocab_size=10,\n",
    "                         maximum_position_encoding=30)\n",
    "p = vectorize_layer(x_test_spaced.iloc[17])\n",
    "model(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "922102cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_vectorized shape: (1280, 29)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "from keras.models import load_model\n",
    "from keras.utils import get_custom_objects\n",
    "\n",
    "get_custom_objects().update({'Encoder': Encoder})\n",
    "# 加载模型\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# # 假设cutWord函数已定义\n",
    "# def cutWord(sequence):\n",
    "#     # 在这里添加cutWord函数的实现\n",
    "#     return sequence\n",
    "\n",
    "# 读取txt文件到DataFrame中\n",
    "x_test_df = pd.read_csv('predicted_data/1280.txt', header=None, names=['seq'])\n",
    "\n",
    "# 预处理数据\n",
    "x_test_spaced = x_test_df.applymap(cutWord)\n",
    "\n",
    "# 创建并适应vectorize_layer（假设你之前已经定义了d_model和vectorize_layer）\n",
    "# d_model =  # 例如，假设d_model是10000\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=d_model, standardize='lower_and_strip_punctuation', split='whitespace', ngrams=None, output_mode='int')\n",
    "vectorize_layer.adapt(x_test_spaced)\n",
    "\n",
    "# 向量化数据\n",
    "x_test_vectorized = vectorize_layer(x_test_spaced)\n",
    "\n",
    "# 确保输入序列的长度一致\n",
    "# 使用pad_sequences确保所有序列具有相同的长度\n",
    "# x_test_vectorized = tf.keras.preprocessing.sequence.pad_sequences(x_test_vectorized, padding='post')\n",
    "\n",
    "# 打印数据形状\n",
    "print(f\"x_test_vectorized shape: {x_test_vectorized.shape}\")\n",
    "\n",
    "# 转换为TensorFlow数据集\n",
    "x_test_dataset = tf.data.Dataset.from_tensor_slices(x_test_vectorized)\n",
    "# print(x_test_dataset.shape)\n",
    "# 批量处理数据集\n",
    "batch_size = 32\n",
    "x_test_dataset = x_test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e01d6378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 3ms/step\n",
      "[2.1006718  2.1977248  0.7809396  ... 1.7995156  0.94963634 0.9523723 ]\n"
     ]
    }
   ],
   "source": [
    "# 进行预测\n",
    "predictions = model.predict(x_test_dataset)\n",
    "\n",
    "# 打印预测结果\n",
    "print(predictions)\n",
    "\n",
    "# 如果需要将预测结果保存到文件中\n",
    "pd.DataFrame(predictions, columns=[\"prediction\"]).to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43609d96788f9f207c5e22c650b4a13ebb7a351aa95d515be949d18331f6c071"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
