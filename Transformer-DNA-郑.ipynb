{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa43a3",
   "metadata": {},
   "source": [
    "# Apply Encoder of Transformer Model (Self Attention) to Explore RNA-Protein Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe31d0-34ea-4d56-8394-e46f02165475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, KBinsDiscretizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import datetime\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a93be1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad2fbca",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86cc08-c411-4ce1-a04d-c82e7a7558f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preprocess = pd.read_csv(\"output2.txt\",header=None)\n",
    "# df_preprocess1 = pd.read_excel(\"fluorescence_w_20799.xlsx\",header=None)\n",
    "# # concat合并df_preprocess和df_preprocess1,在header分别添加promoter和fluorescence\n",
    "# df_preprocess = pd.concat([df_preprocess, df_preprocess1], axis=1)\n",
    "# df_preprocess.columns = ['promoter', 'fluorescence']\n",
    "# df_preprocess.head()\n",
    "# df  = df_preprocess\n",
    "df_preprocess = pd.read_csv(\"data/data2.csv\")\n",
    "df_preprocess1 = pd.read_csv(\"data/bu0.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将df_preprocess和df_preprocess1合并\n",
    "df_preprocess = pd.concat([df_preprocess, df_preprocess1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_preprocess的promoter列和s*列\n",
    "df = df_preprocess[['padded_combined_column','expression_level']]\n",
    "df.head()\n",
    "#df的s*列改为fluorescence\n",
    "df.columns = ['promoter','fluorescence']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d009b3a",
   "metadata": {},
   "source": [
    "## Set hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d15ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=25\n",
    "num_layers=1\n",
    "d_model=9\n",
    "num_heads=3\n",
    "dff=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa0130",
   "metadata": {},
   "source": [
    "## Functions to handle the RNA Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb6b4e-b7bc-4e30-b047-71a74f098685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutWord(x, window=1, length = seq_length):\n",
    "    seq2word = []\n",
    "    for i in range(length):\n",
    "        seq2word.append(x[i*window:i*window+window])\n",
    "    return \" \".join(seq2word)\n",
    "\n",
    "def RKA_bin(x):\n",
    "    if x >= 0.8:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "def MFE_bin(x):\n",
    "    if x >= -14.:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcebc1",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50dce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(df[[\"fluorescence\"]]))\n",
    "\n",
    "df[\"fluorescence_norm\"] = scaler.transform(df[[\"fluorescence\"]])\n",
    "df[[\"fluorescence_logplus1\"]] = df[[\"fluorescence\"]].apply(lambda x: np.log2(1+x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[[\"fluorescence_logplus1\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae9562",
   "metadata": {},
   "source": [
    "## Create dataset for Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18820a02-89ad-45a1-8de1-44bb348c6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"RKA_level\"] = df.RKA.apply(RKA_bin)\n",
    "# df[\"MFE_level\"] = df.RKA.apply(MFE_bin)\n",
    "# target = df[[\"MFE_level\",\"RKA_level\"]].copy()\n",
    "sequences = df[[\"promoter\"]].copy()\n",
    "seq_spaced = sequences.applymap(cutWord)\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=d_model, standardize='lower_and_strip_punctuation', split='whitespace', ngrams=None, output_mode='int')\n",
    "vectorize_layer.adapt(seq_spaced)\n",
    "\n",
    "#Divide to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq_spaced, df[\"fluorescence_logplus1\"], test_size=0.33, random_state=42)\n",
    "#vec_seq = vectorize_layer(X_train)\n",
    "#seq_database = v\n",
    "all_vec_seq = vectorize_layer(seq_spaced)\n",
    "all_vec_seq = tf.data.Dataset.from_tensor_slices(all_vec_seq)\n",
    "#target_database = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "target_database = tf.data.Dataset.from_tensor_slices(df[\"fluorescence_logplus1\"])\n",
    "x_test_database = vectorize_layer(X_test)\n",
    "x_test_database = tf.data.Dataset.from_tensor_slices(x_test_database)\n",
    "y_test_database = tf.data.Dataset.from_tensor_slices(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set = tf.data.Dataset.zip((seq_database, target_database))\n",
    "train_set = tf.data.Dataset.zip((all_vec_seq, target_database))\n",
    "test_dataset = tf.data.Dataset.zip((x_test_database, y_test_database))\n",
    "real_train_set = train_set.batch(128).shuffle(128)\n",
    "test_dataset = test_dataset.batch(50)\n",
    "#next(iter(train_set.batch(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c63d37",
   "metadata": {},
   "source": [
    "## Build Encoder of Transformer model\n",
    "\n",
    "### Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7486e47-19c2-4711-b10d-e75ccf19e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) # Shape = (..., seq_len_q, seq_len_k)\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # calculate matmul_qk_v\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights, matmul_qk\n",
    "\n",
    "#create multi-head attention layer\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        #Because for multi-head, head number * depth = multi-head\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        #Set layers for q, k, v\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) # shape = (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights, matmul_qk= scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights, matmul_qk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51930153",
   "metadata": {},
   "source": [
    "### Fully connected layer and Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1738c8-b834-4375-bf81-bbe3f37cb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        ])\n",
    "        \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        #define layers\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "\n",
    "        attn_output, attn_weight, matmul_qk = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2, attn_weight, matmul_qk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ab76d",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rate = 1/np.power(10000, (2*(i/2))/np.float32(d_model))\n",
    "    return pos*angle_rate\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :]\n",
    "                            ,d_model)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoding(1, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258082e",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b9d93-4b89-4f58-9b9e-5d2dbaa5164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers #how many encoder layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.attention_weights = {}\n",
    "        self.matmul_qks = {}\n",
    "        \n",
    "    def call(self, x, training, mask=None):\n",
    "        #attention_weights = {}\n",
    "        #encoding and position encoding\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block, matmul_qk = self.enc_layers[i](x, training, mask)\n",
    "            self.attention_weights[f'attentions_{i+1}'] = block\n",
    "            self.matmul_qks[f'matmul_qk_{i+1}'] = matmul_qk\n",
    "            \n",
    "        #attention_weights[f'decoder_layer{i+1}_block'] = block\n",
    "        return x, block # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    def get_attention(self):\n",
    "        return self.attention_weights\n",
    "    def get_matmul_qks(self):\n",
    "        return self.matmul_qks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284c0a5",
   "metadata": {},
   "source": [
    "### Test of Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a68b4-b50a-443a-8a4d-0092b80bfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=5, num_heads=1,\n",
    "                         dff=dff, input_vocab_size=10,\n",
    "                         maximum_position_encoding=30)\n",
    "temp_input = vectorize_layer(seq_spaced)\n",
    "#temp_input = tf.random.uniform((3, 24), dtype=tf.int64, minval=0, maxval=200)\n",
    "sample_encoder_output = sample_encoder(temp_input[22:29], training=False, mask=None)\n",
    "sample_encoder_output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ced629",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21f309-13a3-42f8-9ffb-870055dd21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_length, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "    input = tf.keras.Input(shape=(seq_length,))\n",
    "    x , aws= Encoder(num_layers, d_model, num_heads, dff,\n",
    "                input_vocab_size, maximum_position_encoding, rate=0.1)(input)\n",
    "    x = tf.keras.layers.Reshape((seq_length*d_model,))(x)\n",
    "    x = tf.keras.layers.Dense(seq_length, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    output = tf.squeeze(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=input, outputs=output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8b704-a4a7-49be-b52a-bad9eed97073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_length=seq_length, num_layers= num_layers, d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, input_vocab_size=10,\n",
    "                         maximum_position_encoding=30)\n",
    "p = vectorize_layer(seq_spaced.iloc[17])\n",
    "model(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b453f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367f182",
   "metadata": {},
   "source": [
    "## Test our model to see if it can work and get the attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee88c5d-55dc-4a82-94ab-c8c12a161840",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in real_train_set.take(1):\n",
    "    q = model(i[0])\n",
    "att = model.layers[1].get_attention()['attentions_1']\n",
    "qk = model.layers[1].get_matmul_qks()['matmul_qk_1']\n",
    "#forheat = tf.squeeze(att).numpy()\n",
    "sns.heatmap(qk[(0,0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cedcf",
   "metadata": {},
   "source": [
    "## Custom Schedule/ Accuracy/ Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29255630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "#optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "loss_object = loss=tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    #mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    #mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    #loss_ *= mask\n",
    "    #return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "    return tf.reduce_sum(loss_)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = loss_object(real, pred)\n",
    "    #accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    #accuracies = tf.keras.losses.MeanAbsoluteError(real, pred)\n",
    "    #mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    #accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    #accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    #mask = tf.cast(mask, dtype=tf.float32)\n",
    "    #return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "    return tf.reduce_sum(accuracies)\n",
    "\n",
    "train_metric = tfa.metrics.r_square.RSquare()\n",
    "test_metric = tfa.metrics.r_square.RSquare()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "test_accuracy = tf.keras.metrics.Mean(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f60aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints_64_21_pos_郑/new\"\n",
    "ckpt = tf.train.Checkpoint(transformer=model,\n",
    "                           optimizer=optimizer)\n",
    "# 检查点存储管理器\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print('Latest checkpoint restored!!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80d4085f",
   "metadata": {},
   "source": [
    "## Customized Train/Test steps\n",
    "\n",
    "If you face some type mismatch, please use pip install typeguard==2.13.3.\n",
    "typeguard 3.0.0 version was released recently that's the reason why it's not working.\n",
    "\n",
    "Source: https://stackoverflow.com/questions/75759597/typeerror-isinstance-arg-2-must-be-a-type-or-tuple-of-types-in-tensorflow-add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b97135-299c-4559-8941-40c2c000a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "#train_step_signature = [\n",
    "#    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#    tf.TensorSpec(shape=(None), dtype=tf.int64),\n",
    "#]\n",
    "\n",
    "\n",
    "#@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    #inp = inp\n",
    "    target = tar\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inp, training = True)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "\n",
    "    train_metric.update_state(target, predictions)\n",
    "    #result = metric.result()\n",
    "    #train_accuracy(result)\n",
    "    #train_accuracy(accuracy_function(target, predictions))\n",
    "    \n",
    "def test_step(model, inp, tar):\n",
    "    #inp = inp\n",
    "    target = tar\n",
    "    predictions = model(inp)\n",
    "    testloss = loss_function(tar, predictions)\n",
    "    test_loss(testloss)\n",
    "    test_metric.update_state(target, predictions)\n",
    "\n",
    "    #result = metric.result()\n",
    "    #test_accuracy(result)\n",
    "\n",
    "    #train_accuracy(accuracy_function(target, predictions))\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c7f33",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180ff22-68d6-4461-91f9-6a784a977175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "EPOCHS= 500\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "\n",
    "  for (batch, (inp, tar)) in enumerate(real_train_set):\n",
    "    train_step(inp, tar)\n",
    "    if batch % 300 == 0:\n",
    "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_metric.result():.4f}')\n",
    "\n",
    "  with train_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', train_metric.result(), step=epoch)\n",
    "\n",
    "  for (batch, (inp, tar)) in enumerate(test_dataset):\n",
    "    test_step(model, inp, tar)\n",
    "\n",
    "  with test_summary_writer.as_default():\n",
    "    #tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', test_metric.result(), step=epoch)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Test Loss: {}, Test R^2: {}'\n",
    "  print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         test_loss.result(),\n",
    "                         test_metric.result()\n",
    "                         ))\n",
    "  #Print out information\n",
    " \n",
    "\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_metric.result():.4f}')\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb1e38",
   "metadata": {},
   "source": [
    "## Visualize attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"seq_spaced\"] = seq_spaced\n",
    "def visualize_attention(heads, labels, attention):\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "    fig, axs = plt.subplots(1,heads)\n",
    "    plt.set_cmap('bwr')\n",
    "    for i in range(heads):\n",
    "        ax = axs[i]\n",
    "        cax = ax.matshow(attention[i])\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_yticks(range(len(labels)))\n",
    "        #labels = [label for label in R.to_numpy()[0]]\n",
    "        ax.set_xticklabels(labels, rotation=90)\n",
    "        #labels = [label for label in R.to_numpy()[0]]\n",
    "        ax.set_yticklabels(labels)\n",
    "        print(f\"head{i}: {tf.reduce_max(attention[i])}\")\n",
    "        print(f\"head{i}: {tf.reduce_min(attention[i])}\")\n",
    "    cbar = fig.colorbar(cax)\n",
    "\n",
    "def plot_attention_head(sequence, attention, heads):\n",
    "    # The plot is of the attention when a token was generated.\n",
    "    # # The model didn't generate `<START>` in the output. Skip it.\n",
    "    labels = [label for label in sequence.to_numpy()[0]]\n",
    "    #translated_tokens = translated_tokens[1:]\n",
    "    #ax = plt.gca()\n",
    "    visualize_attention(heads = heads, labels=labels, attention=attention)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e11ed",
   "metadata": {},
   "source": [
    "## Test individual sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b54b3d",
   "metadata": {},
   "source": [
    "## Verify the predicted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(all_vec_seq.batch(100))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc33748",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6,5)\n",
    "sns.scatterplot(x=df.fluorescence_logplus1, y=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07484671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R^2\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(df.fluorescence_logplus1, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.fluorescence_logplus1, color = \"orange\")\n",
    "sns.histplot(result, color = \"blue\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43609d96788f9f207c5e22c650b4a13ebb7a351aa95d515be949d18331f6c071"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
